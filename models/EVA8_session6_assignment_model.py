#!/usr/bin/env python
"""
EVA8_session6_assignment_model.py: CNN class definition for EVA8 assignment 6
"""
from __future__ import print_function
import sys
import torch.nn as nn
import torch.nn.functional as F

sys.path.append('./')
dropout_value = 0.05  # Found to be working best for the Session-6 custom


class SeparableConv2d(nn.Module):
    # For this model Depthwise Separable Convolution blocks, implemented as
    # class: SeparableConv2d are used, instead of the normal conv2d
    # kernels as these can help us save parameters (even by a factor of 8+)
    # hence we are free to use more in turn!
    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,
                 padding=0, dilation=1, bias=False):
        super(SeparableConv2d, self).__init__()
        self.convblock = nn.Sequential(
            nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding,
                      dilation=dilation, groups=in_channels, bias=bias),
            nn.Conv2d(in_channels, out_channels, 1, 1, 0, 1, 1, bias=bias),
        )

    def forward(self, x):
        x = self.convblock(x)
        return x


class EVA8_session6_assignment_model(nn.Module):
    """
    model definition used in session 6 to be trained on CIFAR10 dataset.
    Please Note that this model has been structured as:
                            C1-T1-C2-T2-C3-T3-C4-O
    C1, C2 & C3 BLOCKS:
    The underlying structure, each of the C1, C2, C3 (i.e. the conv2d blocks,
    which are followed by a transition layer are made up of 2 conv2d layers:

        first one-- a normal/usual 3x3 Conv2d with padding=1 & stride 1
        &
        second one--a dilated kernel, chosen such that it emulates full-fledged
                    max-pool function under C1 & C2 blocks (i.e. the effective
                    kernel size is 5x5(by having dilation rate =2, padding=2
                    & stride=2, thus under C1 & C2 block positions this layer
                    converts an input of 32x32 to 16x16 @ C1 and 16x16 to
                    8x8 @ C2), But under C3 this though has the effective kernel
                    size still at 5x5 (using dilation rate value=2) but the
                    padding and stride values both have been changed to 1.
                    Thus, for the C3 position, rather than using a perfect
                    max-pool function (which would have changed the input size
                    of 8x8 to 4x4, we instead with this careful choice of
                    effective kernel size =5, padding =1 & stride=1 for this
                    layer, under C3 block position, convert the input-size from
                    8x8 to an output of 6x6 instead (as for the GAP stage & also
                    for any sort of grad-cam assisted class mapping an output
                    size less than 5x5 is not desirable)

    TRANSITION BLOCKS (1, 2 & 3) :
    While all the three transition layers have a normal/usual 1x1 conv2d with
    padding = 0 & stride = 1 (actually these work along with the 2nd layer
    of the C1, C2 & C3 layers as explained earlier.)

    C4 BLOCK: a "capacity boosting" number required  for the output layer (here
    though, the chosen 128 value, is same for earlier layers)

    OUTPUT BLOCK: GAP layer followed by a 1x1 operator (which actually
    resembles a fully-connected(FC) layer in this case. A noteworthy point (
    which relates to the "capacity" element of the overall network (NW)) is
    that the FC (the 1x1 conv2d behaviour here) will work better,
    in generating the final 10 "class-features" to be used by the
    log_softmax.If the inputs to it, have more "features points", i.e. for
    example if we do a 64->10 conversion, vs a  conversion, we can
    expect the 10 class-features (for softmax to decide) generated by a
    128->10 conversion will be more "robust" (as compared to say a 64->10
    conversion)
    Also note a choice of 256->10 can prove to be prone to overfitting, hence
    128->10 choice is optimum.

    Overall, in terms of the receptive field we get an RF of 71x71 for this
    design.Which is greater than the minimum requirement of 44x44.

    """

    def __init__(self, normalization='batch'):
        super(EVA8_session6_assignment_model, self).__init__()
        self.normalization = normalization

        # C1 Block

        # converts the input-size of 32x32-> 16x16
        # first conv2d layer normal 3x3 with padding=1, stride=1
        # second conv2d layer effective 5x5, padding=2, stride=2 (these choices
        # help it emulate the max-pool function exactly)

        # Please Note that the first layer (sometimes called "image-conv2d
        # layer here is a normal 3x3 conv2d with padding=1 & stride 1)
        self.convblock1 = nn.Sequential(
            SeparableConv2d(in_channels=3, out_channels=64,
                            kernel_size=(3, 3), padding=1, bias=False),
            nn.BatchNorm2d(64),
            nn.Dropout(dropout_value),
            nn.ReLU(),
        )  # input:32x32x3, output:32x32x64, RF:3x3

        # Following kernel layer has an effective kernel size of 5
        # due to the dilation-rate of 2, it also has a stride of 2
        # The dilation rate of 2 makes the 3x3 kernel effectively
        # as 5x5 { (kernel-size -1)*dilation_rate + 1 }, chosen
        # so as the 2nd layer of the C1 block itself achieve the
        # receptive field value of 7x7 (along with first layer of C1)
        # this 7x7 is OK for the CIFAR10 dataset to get the edges &
        # gradients (and hence there is a need for a "transition layer"
        # beyond this point, that is where the stride of 2 and
        # effective kernel size of 5x5 help in replacing the max-pool
        # layer option, i.e. 2nd layer of C1 block itself emulates
        # the max-pool function too in this layer position due to
        # it being 5x5 & stride of 2.
        self.convblock2 = nn.Sequential(
            SeparableConv2d(in_channels=64, out_channels=128,
                            kernel_size=(3, 3), stride=2, padding=2,
                            bias=False, dilation=2),
            nn.BatchNorm2d(128),
            nn.Dropout(dropout_value),
            nn.ReLU(),
        )  # input:32x32x64, output:16x16x128, RF:7x7

        # TRANSITION BLOCK 1

        # Please Note that though conventionally, max-pool & 1x1 conv2d layers
        # can be used at the "transition" layer positions, but as the earlier
        # layer had already emulated max pool functionality too, we have a
        # single 1x1 conv2d layer left in this transition layer position.
        self.convblock3 = nn.Sequential(
            SeparableConv2d(in_channels=128, out_channels=64,
                            kernel_size=(1, 1),
                            padding=0, bias=False),
            nn.BatchNorm2d(64),
            nn.Dropout(dropout_value),
            nn.ReLU(),
        )  # input:16x16x128, output:16x16x64, RF:7x7

        # C2 Block

        # converts the input-size of 16x16-> 8x8
        # first conv2d layer normal 3x3 with padding=1, stride=1
        # second conv2d layer effective 5x5, padding=2, stride=2 (these choices
        # help it emulate the max-pool function exactly)

        self.convblock4 = nn.Sequential(
            SeparableConv2d(in_channels=64, out_channels=128,
                            kernel_size=(3, 3),
                            padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.Dropout(dropout_value),
            nn.ReLU(),
        )  # input:16x16x64, output:16x16x128, RF:11x11
        self.convblock5 = nn.Sequential(
            SeparableConv2d(in_channels=128, out_channels=128,
                            kernel_size=(3, 3),
                            stride=(2, 2), padding=2, bias=False,
                            dilation=(2, 2)),
            nn.BatchNorm2d(128),
            nn.Dropout(dropout_value),
            nn.ReLU(),
        )  # input:16x16x128, output:8x8x128, RF:19x19

        # TRANSITION BLOCK 2
        self.convblock6 = nn.Sequential(
            SeparableConv2d(in_channels=128, out_channels=64,
                            kernel_size=(1, 1),
                            padding=0, bias=False),
            nn.BatchNorm2d(64),
            nn.Dropout(dropout_value),
            nn.ReLU(),
        )  # input:8x8x128, output:8x8x64, RF:19x19

        # C3 Block

        # converts the input-size of 8x8-> 6x6 first conv2d layer normal 3x3
        # with padding=1, stride=1 second conv2d layer effective 5x5,
        # padding=1, stride=1 (these choices help it to convert 8x8 to 6x6
        # and NOT to emulate the max-pool function exactly at this layer
        # position)

        self.convblock7 = nn.Sequential(
            SeparableConv2d(in_channels=64, out_channels=128,
                            kernel_size=(3, 3),
                            padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.Dropout(dropout_value),
            nn.ReLU(),
        )  # input:8x8x64, output:8x8x128, RF:27x27
        self.convblock8 = nn.Sequential(
            SeparableConv2d(in_channels=128, out_channels=128,
                            kernel_size=(3, 3),
                            padding=1, bias=False, dilation=(2, 2)),
            nn.BatchNorm2d(128),
            nn.Dropout(dropout_value),
            nn.ReLU(),
        )  # input:8x8x128, output:6x6x128, RF:43x43

        # TRANSITION BLOCK 3
        self.convblock9 = nn.Sequential(
            SeparableConv2d(in_channels=128, out_channels=64,
                            kernel_size=(1, 1),
                            padding=0, bias=False),
            nn.BatchNorm2d(64),
            nn.Dropout(dropout_value),
            nn.ReLU(),
        )  # input:6x6x128, output:6x6x64, RF:43x43

        # C4 Block

        # The so-called "Capacity-booster block" just before the GAP layer
        # to get appropriate numbers of input features for the fully-connected
        # layer following the GAP-layer (that fully connected layer can be a
        # 1x1 conv2d based, as chosen here, post Gap layer.

        self.convblock10 = nn.Sequential(
            SeparableConv2d(in_channels=64, out_channels=128,
                            kernel_size=(3, 3),
                            padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.Dropout(dropout_value),
            nn.ReLU(),
        )  # input:6x6x64, output:6x6x128, RF:51x51

        # OUTPUT BLOCK
        self.gap = nn.Sequential(
            nn.AvgPool2d(kernel_size=6)
        )  # input:6x6x128, output:1x1x128, RF:71x71

        self.convblock11 = nn.Sequential(
            SeparableConv2d(in_channels=128, out_channels=10,
                            kernel_size=(1, 1),
                            padding=0, bias=False),
        )  # input:1x1x128, output:1x1x10,

    def forward(self, x):
        # C1 Block
        x = self.convblock1(x)
        x = self.convblock2(x)
        # TRANSITION BLOCK 1
        x = self.convblock3(x)
        # C2 Block
        x = self.convblock4(x)
        x = self.convblock5(x)
        # TRANSITION BLOCK 2
        x = self.convblock6(x)
        # C3 Block
        x = self.convblock7(x)
        x = self.convblock8(x)
        # TRANSITION BLOCK 3
        x = self.convblock9(x)
        # C4 Block
        x = self.convblock10(x)
        # OUTPUT BLOCK
        x = self.gap(x)
        x = self.convblock11(x)
        # Reshape
        x = x.view(-1, 10)
        return F.log_softmax(x, dim=-1)  # Note torch.nn.CrossEntropyLoss
        # :criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single
        # class
